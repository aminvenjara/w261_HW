{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1\n",
    "W261 - Machine Learning at Scale, Section 2\n",
    "- Amin Venjara\n",
    "- January 18, 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple ways to define big data. One way is the 3 Vs: volume, variety, velocity. \n",
    "    - volume: that is so large it cannot fit in memory on a single computer. A typical laptop can store upto 1TB of data. Laptops have trouble processing training data even when sizes are in the tens of GBs of data. So, big data comes into play when training data sets are >~10-20GBs \n",
    "    - velocity: the best example here is clickstream data that comes in every second which creates a growing store of data to manage and process\n",
    "    - variety: data in a variety of formats, often unstructured, that needs to be brought together to draw insight. \n",
    "\n",
    "In my world of HR analytics, we pay 1 in 6 Americans comprising over 20M records (~1KB / record = 20TB) spread over a variety of systems and platforms. Users HR data is regularly updated as pay data can happen at a weekly, bi-weekly, or monthly basis. Employees are hired, change roles, leave. We recently sought to benchmark compensation for specific roles in specific geographies leveraging this data. Managing the diversity was by far the hardest as bringing together the data and finding a way to marry between different role names was very challenging.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a test data set with T data points\n",
    "\n",
    "**For each data point x* in the test data set compute variance over the variance predictions (50 models\n",
    "give 50 predictions for each data point x*)**\n",
    "\n",
    "**Definitions**\n",
    "- hD(x*) = model prediction (assume 50 training datesets)\n",
    "- h(x*) = Average model prediction\n",
    "- f(x*) = TRUE (Actual function value)\n",
    "- Y* = Observed target data (noisy)\n",
    "\n",
    "**For each data point x* calculate**\n",
    "- Variance = sum(h(x*) – h(x*))^2/50 ## Describes how much h(x*) varies from one training set S to another\n",
    "- Bias: h(x*) – f(x*) ## Describes the average error of h(x*).\n",
    "- Noise: (y* – f(x*))2 ## Describes how much y* varies from f(x*)\n",
    "\n",
    "**Compute Expected prediction error = Variance + Bias^2 + Noise^2**\n",
    "** Find the minimum expected prediction error ** \n",
    "- This can be done via a plot as illustrated below. \n",
    "\n",
    "<img src=\"plot1.png\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!perl -pi -e 's/\\r/\\n/g' enronemail_1h.txt  # to unix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      99 enronemail_1h.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.1. Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print 'done' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Amin Venjara\n",
    "## Description: mapper code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import string\n",
    "count = 0\n",
    "from collections import Counter\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower()) # will only be single word\n",
    "c = Counter()\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        c.update(line.translate(None, string.punctuation).split())\n",
    "    for word in findwords:\n",
    "        print '%s\\t%s'% (word, c[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Amin Venjara\n",
    "## Description: reducer code for HW1.2\n",
    "\n",
    "import sys\n",
    "import re\n",
    "sum = 0\n",
    "    \n",
    "# capture the list of mapped files as a list\n",
    "mapped_files = sys.argv[1:]\n",
    "\n",
    "# stores cumulative count of words across mapped files\n",
    "word_count = {}\n",
    " \n",
    "# input comes from mapper.py\n",
    "for f in mapped_files:\n",
    "    with open (f, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            # remove leading and trailing whitespace\n",
    "            line = line.strip()\n",
    "            \n",
    "            # parse the input we got from mapper.py\n",
    "            word, count = line.split('\\t', 1)\n",
    "\n",
    "            # convert count (currently a string) to int\n",
    "            try:\n",
    "                count = int(count)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                word_count[word] = word_count[word]+count\n",
    "            except:\n",
    "                word_count[word] = count\n",
    "\n",
    "# write the tuples to stdout\n",
    "# Note: they are unsorted\n",
    "for word in word_count.keys():\n",
    "    print '%s\\t%s'% ( word, word_count[word] )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; chmod +x pNaiveBayes.sh;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\"\n",
    "!head enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results. To do so, make sure that mapper.py and reducer.py that performs a single word Naive Bayes classification.**\n",
    "\n",
    "For multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "NOTE: if  “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeld as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimated of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial NB works as follows for a single word:**\n",
    "\n",
    "*P(spam|\"assistance\") = prior_spam x P(\"assistance\"|spam) x count_of_word*\n",
    "\n",
    "So need to compute:\n",
    "- prior_spam = # of spam emails / total number of emails\n",
    "- P (\"assistance\"| SPAM) = # of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM\n",
    "- count_of_word is the number of time the given word appears in the email\n",
    "\n",
    "Alternatively:\n",
    "- P(NOT spam|word in email) = prior_NOTspam x P(word in email |NOT spam) x count_of_word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Amin Venjara\n",
    "## Description: mapper code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "count = 0\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower()) # will only be single word\n",
    "c = Counter()\n",
    "    \n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        email = line.translate(None, string.punctuation).split()\n",
    "        c = Counter(email[2:])\n",
    "        (email_id, SPAM_flag, target_word_count, email_word_count) = (line.split()[0], int(email[1]), c[findwords[0]], sum(c.values()))\n",
    "        print '%s\\t%d\\t%d\\t%d'% (email_id, SPAM_flag, target_word_count, email_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/Applications/anaconda/bin/python\n",
    "## reducer.py\n",
    "## Author: Amin Venjara\n",
    "## Description: reducer code for HW1.3\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "sum = 0\n",
    "    \n",
    "# capture the list of mapped files as a list\n",
    "mapped_files = sys.argv[1:]\n",
    "\n",
    "####################\n",
    "# gather the data  #\n",
    "####################\n",
    "\n",
    "# stores email data across mapped files\n",
    "col_names = ['email_id', 'SPAM_flag', 'target_word_count', 'email_word_count']\n",
    "email_parser = pd.DataFrame()\n",
    "     \n",
    "# create dataframe that combines data from mapper.py\n",
    "for f in mapped_files:\n",
    "    with open (f, \"r\") as myfile:\n",
    "        email_parser = email_parser.append(pd.read_csv(myfile, sep='\\t', names=col_names), ignore_index = True)\n",
    "        \n",
    "#print email_parser\n",
    "\n",
    "####################\n",
    "# process the data #\n",
    "####################\n",
    "\n",
    "# create a df that select just SPAM emails\n",
    "df_SPAM_emails = email_parser[email_parser.SPAM_flag == 1] \n",
    "\n",
    "total_emails = len(email_parser.index)\n",
    "print \"total_emails = \", total_emails\n",
    "\n",
    "SPAM_emails = len(df_SPAM_emails.index)\n",
    "print \"SPAM_emails = \", SPAM_emails\n",
    "\n",
    "target_word_count_in_SPAM_emails = df_SPAM_emails['target_word_count'].sum()\n",
    "print \"target_word_count_in_SPAM_emails = \", target_word_count_in_SPAM_emails\n",
    "\n",
    "SPAM_word_count = df_SPAM_emails['email_word_count'].sum()\n",
    "print \"SPAM_word_count = \", SPAM_word_count\n",
    "\n",
    "#compute the elements of the bayesian calculation\n",
    "prior_spam = float(SPAM_emails)/float(total_emails)\n",
    "print prior_spam\n",
    "\n",
    "prob_targetword_given_SPAM = float(target_word_count_in_SPAM_emails) / float(SPAM_word_count)\n",
    "print prob_targetword_given_SPAM\n",
    "\n",
    "output = email_parser[['email_id', 'SPAM_flag']]\n",
    "output['classification'] = (prior_spam*prob_targetword_given_SPAM*email_parser['target_word_count']).round().astype(int)\n",
    "\n",
    "####################\n",
    "# output the data  #\n",
    "####################\n",
    "print output.to_csv(sep='\\t', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; chmod +x pNaiveBayes.sh;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_emails =  100\r\n",
      "SPAM_emails =  44\r\n",
      "target_word_count_in_SPAM_emails =  8\r\n",
      "SPAM_word_count =  18283\r\n",
      "0.44\r\n",
      "0.000437564951047\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t0\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t0\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"assistance\"\n",
    "!head -100 enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results. To do so, make sure that**\n",
    "\n",
    "   - mapper.py counts all occurrences of a list of words, and\n",
    "   - reducer.py \n",
    "\n",
    "**performs the multiple-word multinomial Naive Bayes classification via the chosen list.\n",
    "No smoothing is needed in this HW.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Amin Venjara\n",
    "## Description: mapper code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "count = 0\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower()) # will only be single word\n",
    "c = Counter()\n",
    "    \n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        email = line.translate(None, string.punctuation).split()\n",
    "        c = Counter(email[2:])\n",
    "        (email_id, SPAM_flag, target_word_count, email_word_count) = (line.split()[0], int(email[1]), c[findwords[0]], sum(c.values()))\n",
    "        print '%s\\t%d\\t%d\\t%d'% (email_id, SPAM_flag, target_word_count, email_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/Applications/anaconda/bin/python\n",
    "## reducer.py\n",
    "## Author: Amin Venjara\n",
    "## Description: reducer code for HW1.4\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "sum = 0\n",
    "    \n",
    "# capture the list of mapped files as a list\n",
    "mapped_files = sys.argv[1:]\n",
    "\n",
    "####################\n",
    "# gather the data  #\n",
    "####################\n",
    "\n",
    "# stores email data across mapped files\n",
    "col_names = ['email_id', 'SPAM_flag', 'target_word_count', 'email_word_count']\n",
    "email_parser = pd.DataFrame()\n",
    "     \n",
    "# create dataframe that combines data from mapper.py\n",
    "for f in mapped_files:\n",
    "    with open (f, \"r\") as myfile:\n",
    "        email_parser = email_parser.append(pd.read_csv(myfile, sep='\\t', names=col_names), ignore_index = True)\n",
    "        \n",
    "#print email_parser\n",
    "\n",
    "####################\n",
    "# process the data #\n",
    "####################\n",
    "\n",
    "# create a df that select just SPAM emails\n",
    "df_SPAM_emails = email_parser[email_parser.SPAM_flag == 1] \n",
    "\n",
    "total_emails = len(email_parser.index)\n",
    "print \"total_emails = \", total_emails\n",
    "\n",
    "SPAM_emails = len(df_SPAM_emails.index)\n",
    "print \"SPAM_emails = \", SPAM_emails\n",
    "\n",
    "target_word_count_in_SPAM_emails = df_SPAM_emails['target_word_count'].sum()\n",
    "print \"target_word_count_in_SPAM_emails = \", target_word_count_in_SPAM_emails\n",
    "\n",
    "SPAM_word_count = df_SPAM_emails['email_word_count'].sum()\n",
    "print \"SPAM_word_count = \", SPAM_word_count\n",
    "\n",
    "#compute the elements of the bayesian calculation\n",
    "prior_spam = float(SPAM_emails)/float(total_emails)\n",
    "print prior_spam\n",
    "\n",
    "prob_targetword_given_SPAM = float(target_word_count_in_SPAM_emails) / float(SPAM_word_count)\n",
    "print prob_targetword_given_SPAM\n",
    "\n",
    "output = email_parser[['email_id', 'SPAM_flag']]\n",
    "output['classification'] = (prior_spam*prob_targetword_given_SPAM*email_parser['target_word_count']).round().astype(int)\n",
    "\n",
    "####################\n",
    "# output the data  #\n",
    "####################\n",
    "print output.to_csv(sep='\\t', header = False, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod +x mapper.py; chmod +x reducer.py; chmod +x pNaiveBayes.sh;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_emails =  100\r\n",
      "SPAM_emails =  44\r\n",
      "target_word_count_in_SPAM_emails =  0\r\n",
      "SPAM_word_count =  18283\r\n",
      "0.44\r\n",
      "0.0\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t0\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t0\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"enlargementWithATypo\"\n",
    "!head -100 enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_emails =  100\r\n",
      "SPAM_emails =  44\r\n",
      "target_word_count_in_SPAM_emails =  3\r\n",
      "SPAM_word_count =  18283\r\n",
      "0.44\r\n",
      "0.000164086856643\r\n",
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t0\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t0\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 4 \"valium\"\n",
    "!head -100 enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
